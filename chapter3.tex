\section{Stopping Problem}
With an estimate and associated variance for the instantaneous energetic cost, the determination for when to stop measuring is known as a finite-horizon stopping problem. Formally, the stopping problem is defined with

\begin{table}[h]
\centering
\begin{tabular}{ |c  c| }
  \hline&\\
  $N$ & Finite horizon\\
  &\\
  $X_t$ & State at time $t$\\
  &\\
  $P(X_t \vert X_{t-1})$ & State transitions, typically Markovian\\
  &\\
  $\lambda$ & Discount Factor $\in (0, 1]$\\
  &\\
  $r(X)$ & Bounded reward function for continuing at state $X$\\
  &\\
  $g(X)$ & Bounded reward function for stopping at state $X$\\
  &\\
  \hline
\end{tabular}
\caption{Setup of finite horizon stopping problem}
\label{tab:stopping}
\end{table}

The objective is to find $\tau$ that maximizes
\begin{align}
  V(x) = \max_{\tau: \tau \leq N} \mathbb{E}[\lambda^{\tau} g(X_{\tau}) + \sum_{i=0}^{\tau-1} \lambda^{i}r(X_j) \vert X_0 = x]
\end{align}

The value function can be computed via backward induction by defining
\begin{align}
\begin{split}
  J_N(x) &= g(x)\\
  J_n(x) &= max\{g(x), r(x) + \lambda\mathbb{E}_{P(y\vert x)}[J_{n+1}(y)]\},
\end{split}
\end{align}
then $V(x) = J_0(x)$ and the optimal stopping time is
\begin{align}
  \tau = \min_{t \geq 0}J_t(X_t) = g(X_t)
\end{align}
For a derivation of the optimal stopping time, see Appendix \ref{thm:ost}. We aim to formulate the stopping problem in terms of the metabolic estimator. Let $N$ be the maximum measurements allowed at a parameter evaluation, $\hat{x}_t \sim \mathcal{N}(\mu_{x_t}, \sigma^2_{x_t})$ the cost estimate and associated variance for the current evaluation, and $\hat{x}^* \sim \mathcal{N}(\mu_{x^*}, \sigma^2_{x^*})$ the cost estimate and variance for the best parameter setting found thus far as defined by the Gaussian process. As Bayesian optimization starts with an exploration phase, $\hat{x}^*$ will always be defined (Algorithm \ref{alg:bayesopt}).

\section{$\sigma$-scaled Offset}
The $\sigma$-scaled offset model directly embeds $\hat{x}_t$ and $\hat{x}^*$ into the stopping problem. The best estimate of the difference in energetic costs is given by the following Gaussian distribution
\begin{align}
\begin{split}
  \hat{x}_t - \hat{x}^* &\sim \mathcal{N}(\mu, \sigma^2)\\
  \mu &= \mu_{x_t} - \mu_{x^*}\\
  \sigma^2 &= \sigma^2_{x_t} + \sigma^2_{x^*}
\end{split}
\end{align}

The stopping problem state is then the estimate difference, with a non-Markovian transition model of independent draws from the same Gaussian distribution. The lower $x_t$ seems relative to $x^*$, the higher the reward function incentivizes continuing. In summary,

\begin{align}
\begin{split}
  X_t &= \mu\\
  P(X_t \vert X_{t-1}) &= P(X_t) \sim \mathcal{N}(\mu, \sigma^2)\\
  r(x) &= K\sigma-x\\
  g(x) &= 0,
\end{split}
\end{align}
where $K$ is a $\sigma$-scaled offset to provide some risk averseness to the estimate. The optimal stopping point for this formulation can be characterized as the first time $X_t > K\sigma$. As more measurements are taken and $P_{x}$ decreases, $\sigma^2$ decreases the offset accordingly.

As shown in Section \ref{sec:ukfcovar}, underestimating $P_w$ can cause the estimator to exhibit high volatility. Additionally, since a subsequent state covariance update $P_x(t+1)$ is bounded by $P_w$, a lower $P_w$ will also cause $\sigma^2$ to be underestimated. With the $\sigma$-scaled offset model, underestimating the noisiness of observations could potentially cause promising parameter settings to stop measuring early as there is no fault tolerance for crossing the threshold. 

\section{Gittins Index}
A more fault-tolerant model is drawn from literature related to the multi-armed bandit problem (MAB), which represents a sequential decision making problem where at any given time a player must select from different options (or arms) that then responds with a variable reward. Historically this has been used in a clinical context, where a physician must select from one of several different treatment options when presented a patient and receives binary rewards of either successes or failures \citep{Villar2015}. A stopping problem has also been described as a one-armed bandit problem \citep{FergusonStopping2006}.

In our case, successes and failures can be based on $\mu, \sigma^2$ using the Probability of Improvement (PI) metric
\begin{align}
  PI(\mu, \sigma^2) = \Phi(\frac{\mu}{\sigma}),
\end{align}

where $\Phi$ is the CDF for a standard normal distribution. A success can then be defined as $PI(\mu, \sigma^2) > T$ for some defined risk tolerance measure $T$. As $\sigma^2$ decreases over more measurements, $PI$ becomes less influenced by $\sigma^2$ and more by $\mu$. 

The stopping problem state follows a Beta distribution where parameters $\alpha,\beta$ correspond to the count of successes and failures, and transitions follow the posterior of the Beta distribution. In summary,
\begin{align}
\begin{split}
  X_t &= (\alpha_t + \alpha_0, \beta_t + \beta_0) \\
  P(X_{t+1}) &=
  \begin{cases}
      (\alpha_t + \alpha_0 + 1, \beta_t + \beta_0)
      \\ \tab\text{w.p. } \frac{\alpha_t + \alpha_0}{\alpha_t + \alpha_0 + \beta_t + \beta_0}\\
      (\alpha_t + \alpha_0, \beta_t + \beta_0 + 1)
      \\ \tab\text{w.p. } \frac{\beta_t + \beta_0}{\alpha_t + \alpha_0 + \beta_t + \beta_0}
  \end{cases}\\
  r((\alpha_t + \alpha_0, \beta_t + \beta_0)) &= \frac{\alpha_t + \alpha_0}{\alpha_t + \alpha_0 + \beta_t + \beta_0},\\
\end{split}
\end{align}
where $\alpha_0,\beta_0$ are smoothing priors and $\alpha_t,\beta_t$ are successes and failures up to time $t$.

Gittins and Jones showed that an optimal policy could be constructed by calculating a value called the Gittins Index \citep{doi:10.1002/9781118596333.ch24}. Let $g(x) = K$ for some constant $K$, then the Gittins Index is defined as
\begin{align}
  \nu(x) = (1-\lambda)\min_{K} \{J_0(x) = K\}
\end{align}
The Gittins Index is generally used for infinite horizon stopping problems, with $g(x)=\frac{\nu(x)}{1-\lambda}$ representing an infinite geometric sum to compensate for all future rewards. By setting $\lambda=1-\frac{1}{N}$, $\forall x \; 0 \leq \nu(x) \leq 1$.

In the case of a multi-armed bandit problem, the optimal policy is to play the arm with the highest Gittins Index. \citet{FergusonStopping2006} shows that the stopping problem, or the one-armed bandit problem, is equivalent to having a second arm that provides a constant reward. The optimal policy then corresponds to playing the arm as long as its Gittins Index is above a specified threshold $V$. As $T$ and $V$ are correlated, one of the parameters can be fixed (such as $T=0.5$) while the other is tuned to a preferred risk tolerance for expected success rate. 

This approach essentially acts as a low-pass filter for a potentially noisy metabolic estimator. The choice for $V$ can similarly be thought of in terms of a Gittins Index 
\begin{align}
\begin{split}
  \nu((\alpha, \beta)) &= V\\
  \alpha + \beta &= N,
\end{split}
\end{align}
where $\alpha, \beta$ are successes and failures over the maximum duration. This is in essence asking the question: "If I were to continue measuring this parameter setting, what is the minimum number of successes that I would need to warrant the cost of experimental time?".

\section{Adaptive Thresholds}
Both the $\sigma$-offset and Gittins model have a risk tolerance threshold that may be difficult to tune across subjects. Another approach is to use the exploration phase of Bayesian optimization to create a schedule for the thresholds adapted to the subject's measurements. A simple approach would be to gauge the trajectory of thresholds if the stopping problem were run on the exploration phase, and accordingly use the most risk averse during the optimization. Let $M$ and $\Sigma$ be $\vert \mathbb{E}\vert \times N$ matrices providing traces where $M_{ij}, \Sigma_{ij}$ correspond to $\mu_{x_j}, \sigma^2_{x_j}$ for exploration point $i$ and measurement $j$. For each exploration point, the $\hat{x}^*$ distribution can be the final distribution, $(M_{iN}, \Sigma_{iN})$, to estimate the magnitude of thresholds when comparing similar cost estimates. Algorithm $\ref{alg:adapthresh}$ details the calculation of a threshold vector $A$.

\begin{algorithm}[t]
\caption{Determining threshold levels for either model based on subject's exploration data. With the $\sigma$-offset, the higher threshold is more risk averse, while with the Gittins model the lower is.}
\label{alg:adapthresh}
\begin{algorithmic}
\State \textkeyword{Init $\vert\mathbb{E}\vert \times N$ Matrix } $X$
\State \textkeyword{Init $N$-Vector } $A$
\State \textkeyword{Define Window } $W$
\For{i=1}{\vert \mathbb{E} \vert}
  \State $\alpha = \alpha_0, \beta = \beta_0$
  \For{j=1}{N}
    \State $\mu = M_{ij} - M_{iN}$
    \State $\sigma^2 = \Sigma_{ij} + \Sigma_{iN}$
    \If{Offset}
      \State $X_{ij} = \frac{\mu}{\sigma}$
    \EndIf
    \If{Gittins}
      \State $\alpha = \alpha + \mathbb{1}_{PI(\mu, \sigma^2) \geq T}$
      \State $\beta = \beta + \mathbb{1}_{PI(\mu, \sigma^2) < T}$
      \State $X_{ij} = \nu((\alpha, \beta))$
    \EndIf
  \End
\End
\For{i=1}{N}
  \If{Offset}
    \State $A_{i} = MAX\{X_{rc}\vert 1 \leq r \leq \vert\mathbb{E}\vert, i-W \leq c \leq i+W\}$
  \EndIf
  \If{Gittins}
    \State $A_{i} = MIN\{X_{rc}\vert 1 \leq r \leq \vert\mathbb{E}\vert, i-W \leq c \leq i+W\}$
  \EndIf
\End
\State Return $A$
\end{algorithmic}
\end{algorithm}

\section{Simulations}
Simulations were run on four optimization functions with Gaussian noise proportional to standard deviation added to measurements. Both the $\sigma$-offset and Gittins models were tested each with risk thresholds $T=[0, 0.25, 0.5]$. The case of $T=0$ for the Gittins model is equivalent to a fixed measurement window and can be used as the point of comparison. A Gaussian state estimate was continually reconciled with new observations, equivalent to a Kalman filter with the identity function plus additive noise for both $F$ and $H$, and measurement noise was tuned to different scenarios $P_w=[\frac{\sigma^2}{10}, \sigma^2, 10\sigma^2]$. The same measurements from exploration points were provided for each function under all scenarios.

For time $t$, let $x_t$ be the GP's estimated minimum point. The figures chart
\begin{align}
  y_t = \frac{f(x_t) - f(x^*)}{f(x_0) - f(x^*)},
\end{align}
where $f$ is the true, noiseless function and $f(x^*)$ is the function minimum. Results from simulations are provided in Appendix \ref{ch:append-sims}. 

\begin{table}[t]
\centering
\begin{tabular}{ |c | c | c | c | c | }
  \hline&&&&\\
  Function & Domain & Minimum & Mean & Std\\
  &&&&\\
  \hline
  &&&&\\
  Hartmann-6 & $[0,1]^6$ & -3.322 & -0.259 & 0.383\\
  &&&&\\
  Ackley & $[-32.768, 32.768]^4$ & 0 & 20.882 & 1.027\\
  &&&&\\
  Levy & $[-10, 10]^4$ & 0 & 42.544 & 27.939\\
  &&&&\\
  Branin & $([-5, 10],[0, 15])$ & 0.398 & 54.452 & 51.129\\
  &&&&\\
  \hline
\end{tabular}
\caption{Optimization functions for simulations}
\label{tab:optfunc}
\end{table}

